{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e08dfcb",
   "metadata": {},
   "source": [
    "### Read in the data and do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aadeffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "x = torch.tensor(train.drop(columns='label').values, dtype=torch.float32)\n",
    "y = torch.tensor(train['label'], dtype=torch.long)\n",
    "\n",
    "test_x = torch.tensor(test.values, dtype=torch.float32)\n",
    "\n",
    "# normalize\n",
    "x = x - 127.5 # center around 0 \n",
    "x = x / 127.5 # scale to range [-1, 1]\n",
    "\n",
    "test_x = test_x - 127.5\n",
    "test_x = test_x / 127.5\n",
    "\n",
    "n_labels = len(torch.unique(y))\n",
    "n_samples = x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0f219",
   "metadata": {},
   "source": [
    "### Let's plot a digit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef2096c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHDElEQVR4nO3cPY4UVwBG0ekRAUIklFjGLIKYVRCwAlQxa6hFgVgDKWEtAY1oJ9aVZQ92V3uq+u+cmEc9WZ65foG/3X6/398BwN3d3f2pLwDA+RAFACIKAEQUAIgoABBRACCiAEBEAYC8OPQP7na7Ne9xMfxz4JL4f1P5q0P+ffBSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAOXgQD7g8Ww04Gt67Hl4KAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgNz2It9VYGFy7LX+WjO+ty0sBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADITa+kApdnq0XWW11j9VIAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgC5mkG8rUaygNuw5e+Ucxrf81IAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgC5mkE8uBTDMBx17uXLl4vPPD4+Lj4zz/PiM/w/5zTo6aUAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQBydoN45zQMBefkx48fm3zn1atXi8/8/PlzhZtwCl4KAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgZzeIB5dkGIbFZ+Z5XuEmT3v79u3iMwbxbpuXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkFVXUne73Zp/PfAfvnz5svjMw8PDCjfhUngpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGArDqIB5dkGIbFZ+Z5XuEmTxvHcfGZjx8/rnATrpmXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyMGDeLvdbs17ACt4eHg49RW4MF4KAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgBw/iwbX78OHDJt8Zx3GT78AxvBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBYSYU/vXv37tRX+FfTNJ36CtwALwUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABCDeJy9YRgWn3nz5s3iM+/fv198ZhzHxWfgnHkpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAGMRjM8cM2x3r+/fvm31rqWmaTn0F+C0vBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEIN4nL15njf5zjiOi88Yt+PaeCkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYAYxOMowzCc+grACrwUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAWEllM/M8b/atcRwXn5mm6fkvAhfGSwGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAMQgHnefPn1afObr168r3AQ41H6/X+Xv9VIAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgAxiMdRvn37ttm3jhnsm6bp+S8CN8BLAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAxCDelTlmPO4Yv379Wnzm/t5/g8BT9vv9qa8QP6UARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAG8a7M69evF5/5/PnzCjf5p2PH+qZpet6LAL/lpQBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAGS33+/3h/zB+3v9uFaPj4+bfGccx6POWUnlUhz46/RkDrmf3/QARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAHD+Ltdru177LpdwCem0E8AK6KKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQF6c+gJ/t+WglPE94HfOfdxuLV4KAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgZzeIt6WtBq8M7wGXwksBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADITa+kbmWrNda7O4us8JQtfwYvnZcCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIQbwrs9Xwl+E9uE5eCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIAcP4m01tAbA6XgpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQPwCCB5K3Q0spbAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = to_pil_image(x[2].reshape(28, 28))  # Reshape to 2D (28x28)\n",
    "def plot_digit(flattened_img):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_digit(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d9322",
   "metadata": {},
   "source": [
    "### Augment the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92339f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m final_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     rotated_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrotation_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     rotated_batch \u001b[38;5;241m=\u001b[39m rotated_batch\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(rotated_batch), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     final_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rotated_batch, final_batch])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "\n",
    "rotation_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-30, 30)),\n",
    "    transforms.ToTensor()  \n",
    "])\n",
    "\n",
    "final_batch = torch.tensor([])\n",
    "final_label = torch.tensor([], dtype=torch.long)\n",
    "for i in range(5):\n",
    "    rotated_batch = torch.stack([rotation_transform(to_pil_image(img.reshape(28, 28))) for img in x])\n",
    "    rotated_batch = rotated_batch.reshape(len(rotated_batch), -1)\n",
    "    final_batch = torch.cat([rotated_batch, final_batch])\n",
    "    final_label = torch.cat([y, final_label])\n",
    "    \n",
    "\n",
    "\n",
    "x = torch.cat([x, final_batch])\n",
    "y = torch.cat([y, final_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37941624",
   "metadata": {},
   "source": [
    "### Let's define our neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d08fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "                nn.Dropout(0.25))\n",
    "            \n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "                nn.Dropout(0.25))\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1600, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, \n",
    "            num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        out = self.layer1(x) # B, filters, height, width\n",
    "        assert out.shape == (batch_size, 32, 14, 14)\n",
    "        out = self.layer2(out)\n",
    "        assert out.shape == (batch_size, 64, 5, 5)\n",
    "        logits = self.linear(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "model = ConvNet(n_labels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6798f",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c583bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.37305540903060774, Acc: 0.8763888888888889, Val Acc: 0.9121031746031746\n",
      "Epoch 1, Loss: 0.2388270163350187, Acc: 0.9218298059964727, Val Acc: 0.9261507936507937\n",
      "Epoch 2, Loss: 0.2114572149439944, Acc: 0.930405643738977, Val Acc: 0.9302380952380952\n",
      "Epoch 3, Loss: 0.19619371396176793, Acc: 0.9353880070546737, Val Acc: 0.9312698412698412\n",
      "Epoch 4, Loss: 0.18597817738163508, Acc: 0.939157848324515, Val Acc: 0.9349603174603175\n",
      "Epoch 5, Loss: 0.13570838219706408, Acc: 0.9546031746031746, Val Acc: 0.9453571428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 30\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.1, random_state=1337)\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move validation data to GPU\n",
    "train_x, val_x = train_x.to(device), val_x.to(device)\n",
    "train_y, val_y = train_y.to(device), val_y.to(device)\n",
    "\n",
    "model.train() # set model to training mode\n",
    "model = model.to(device)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    preds = []\n",
    "    losses = []\n",
    "    for k in range(len(train_x) // batch_size):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start = k * batch_size\n",
    "        end = start + batch_size\n",
    "        sample = train_x[start:end]\n",
    "        sample = sample.view(-1, 1, 28, 28)\n",
    "        logits = model(sample)\n",
    "        loss = F.cross_entropy(logits, train_y[start:end])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds.extend(logits.argmax(dim=1).tolist())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    acc = len([pred for pred, label in zip(preds, train_y) if pred == label])/len(train_x)\n",
    "    \n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for k in range(len(val_x) // batch_size):\n",
    "            start = k * batch_size\n",
    "            end = start + batch_size\n",
    "            sample = val_x[start:end]\n",
    "            sample = sample.view(-1, 1, 28, 28)\n",
    "            logits = model(sample)\n",
    "            val_preds.extend(logits.argmax(dim=1).tolist())\n",
    "        \n",
    "    val_acc = len([pred for pred, label in zip(val_preds, val_y) if pred == label])/len(val_x)\n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {i}, Loss: {sum(losses)/len(losses)}, Acc: {acc}, Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166a900",
   "metadata": {},
   "source": [
    "### Kaggle submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f4d28f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "model.eval() # set model to eval mode\n",
    "with torch.no_grad():\n",
    "    test_steps = len(test_x) // batch_size\n",
    "    preds = [] \n",
    "    for j in range(test_steps):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        sample = test_x[start:end]\n",
    "        sample = sample.view(-1, 1, 28, 28)\n",
    "        logits = model(sample)\n",
    "        \n",
    "        preds.extend(logits.argmax(dim=1).tolist())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({\"ImageID\": list(range(1, 1+len(preds))), \"Label\": preds})\n",
    "    df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
