{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e08dfcb",
   "metadata": {},
   "source": [
    "### Read in the data and do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aadeffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "x = torch.tensor(train.drop(columns='label').values, dtype=torch.float32)\n",
    "y = torch.tensor(train['label'], dtype=torch.long)\n",
    "\n",
    "test_x = torch.tensor(test.values, dtype=torch.float32)\n",
    "\n",
    "# normalize\n",
    "x = x - 127.5 # center around 0 \n",
    "x = x / 127.5 # scale to range [-1, 1]\n",
    "\n",
    "test_x = test_x - 127.5\n",
    "test_x = test_x / 127.5\n",
    "\n",
    "n_labels = len(torch.unique(y))\n",
    "n_samples = x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0f219",
   "metadata": {},
   "source": [
    "### Let's plot a digit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2096c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x[200].reshape(28, 28)  # Reshape to 2D (28x28)\n",
    "def plot_digit(flattened_img):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_digit(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d9322",
   "metadata": {},
   "source": [
    "### Augment the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92339f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m final_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     rotated_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([transform(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     17\u001b[0m     rotated_batch \u001b[38;5;241m=\u001b[39m rotated_batch\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(rotated_batch), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     final_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rotated_batch, final_batch])\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m final_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     rotated_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     17\u001b[0m     rotated_batch \u001b[38;5;241m=\u001b[39m rotated_batch\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(rotated_batch), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     final_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rotated_batch, final_batch])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1559\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_pre_hooks\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "\n",
    "# transformations \n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-30, 30)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.0)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33))\n",
    "])\n",
    "\n",
    "final_batch = torch.tensor([])\n",
    "final_label = torch.tensor([], dtype=torch.long)\n",
    "for i in range(5):\n",
    "    rotated_batch = torch.stack([transform(img.reshape(1, 28, 28)) for img in x])\n",
    "    rotated_batch = rotated_batch.reshape(len(rotated_batch), -1)\n",
    "    final_batch = torch.cat([rotated_batch, final_batch])\n",
    "    final_label = torch.cat([y, final_label])\n",
    "    \n",
    "\n",
    "\n",
    "x = torch.cat([x, final_batch])\n",
    "y = torch.cat([y, final_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37941624",
   "metadata": {},
   "source": [
    "### Let's define our neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d08fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "                nn.Dropout(0.25))\n",
    "            \n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "                nn.Dropout(0.25))\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1600, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, \n",
    "            num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        out = self.layer1(x) # B, filters, height, width\n",
    "        assert out.shape == (batch_size, 32, 14, 14)\n",
    "        out = self.layer2(out)\n",
    "        assert out.shape == (batch_size, 64, 5, 5)\n",
    "        logits = self.linear(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "model = ConvNet(n_labels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6798f",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c583bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.37305540903060774, Acc: 0.8763888888888889, Val Acc: 0.9121031746031746\n",
      "Epoch 1, Loss: 0.2388270163350187, Acc: 0.9218298059964727, Val Acc: 0.9261507936507937\n",
      "Epoch 2, Loss: 0.2114572149439944, Acc: 0.930405643738977, Val Acc: 0.9302380952380952\n",
      "Epoch 3, Loss: 0.19619371396176793, Acc: 0.9353880070546737, Val Acc: 0.9312698412698412\n",
      "Epoch 4, Loss: 0.18597817738163508, Acc: 0.939157848324515, Val Acc: 0.9349603174603175\n",
      "Epoch 5, Loss: 0.13570838219706408, Acc: 0.9546031746031746, Val Acc: 0.9453571428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 30\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.1, random_state=1337)\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move validation data to GPU\n",
    "train_x, val_x = train_x.to(device), val_x.to(device)\n",
    "train_y, val_y = train_y.to(device), val_y.to(device)\n",
    "\n",
    "model.train() # set model to training mode\n",
    "model = model.to(device)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    preds = []\n",
    "    losses = []\n",
    "    for k in range(len(train_x) // batch_size):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start = k * batch_size\n",
    "        end = start + batch_size\n",
    "        sample = train_x[start:end]\n",
    "        sample = sample.view(-1, 1, 28, 28)\n",
    "        logits = model(sample)\n",
    "        loss = F.cross_entropy(logits, train_y[start:end])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds.extend(logits.argmax(dim=1).tolist())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    acc = len([pred for pred, label in zip(preds, train_y) if pred == label])/len(train_x)\n",
    "    \n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for k in range(len(val_x) // batch_size):\n",
    "            start = k * batch_size\n",
    "            end = start + batch_size\n",
    "            sample = val_x[start:end]\n",
    "            sample = sample.view(-1, 1, 28, 28)\n",
    "            logits = model(sample)\n",
    "            val_preds.extend(logits.argmax(dim=1).tolist())\n",
    "        \n",
    "    val_acc = len([pred for pred, label in zip(val_preds, val_y) if pred == label])/len(val_x)\n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {i}, Loss: {sum(losses)/len(losses)}, Acc: {acc}, Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166a900",
   "metadata": {},
   "source": [
    "### Kaggle submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f4d28f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "model.eval() # set model to eval mode\n",
    "with torch.no_grad():\n",
    "    test_steps = len(test_x) // batch_size\n",
    "    preds = [] \n",
    "    for j in range(test_steps):\n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        sample = test_x[start:end]\n",
    "        sample = sample.view(-1, 1, 28, 28)\n",
    "        logits = model(sample)\n",
    "        \n",
    "        preds.extend(logits.argmax(dim=1).tolist())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({\"ImageID\": list(range(1, 1+len(preds))), \"Label\": preds})\n",
    "    df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
